{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import random\n",
    "from scipy.optimize.slsqp import approx_jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaussianProcessRegression(object):\n",
    "    # カーネル関数とノイズの精度パラメータの初期化\n",
    "    def __init__(self, kernel, beta=1.):\n",
    "        self.kernel = kernel\n",
    "        self.beta = beta #論文ではγ\n",
    "\n",
    "    # カーネル関数のパラメータ推定を行わずに回帰\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.K = np.zeros([len(x),len(x)])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                self.K[i][j] = self.kernel(x[i],x[j])\n",
    "\n",
    "    # カーネル関数のパラメータの推定を行う回帰\n",
    "    def fit_kernel(self, x, t, learning_rate=0.1, iter_max=10000):\n",
    "        for i in range(iter_max):\n",
    "            params = self.kernel.get_params()\n",
    "            # カーネル関数の今のパラメータで回帰\n",
    "            self.fit(x, t)\n",
    "            # 対数エビデンス関数をパラメータで微分\n",
    "            gradients = self.kernel.derivatives(*np.meshgrid(x, x))\n",
    "            # パラメータの更新量を計算 PRML式(6.70)\n",
    "            updates = np.array(\n",
    "                [-np.trace(self.precision.dot(grad)) + t.dot(self.precision.dot(grad).dot(self.precision).dot(t)) for grad in gradients])\n",
    "            # パラメータを更新\n",
    "            self.kernel.update_parameters(learning_rate * updates)\n",
    "            # パラメータの更新量が小さければ更新をやめる\n",
    "            \n",
    "            if np.allclose(params, self.kernel.get_params()):\n",
    "                break\n",
    "        else:\n",
    "            # 既定の更新回数だけ更新してもパラメータの更新量が小さくない場合以下の文を出力\n",
    "            print(\"parameters may not have converged\")\n",
    "            \n",
    "    def kernels(self,x):\n",
    "        return np.array([self.kernel(x,_x) for _x in Xt]).T\n",
    "    \n",
    "    def kernels_dif(self,x):\n",
    "        params = self.kernel.get_params()\n",
    "        return np.array([- params[1]*(self.x[i]-x)*params[0]**2 *np.exp(-0.5*params[1]*(self.x[i]-x)**2) for i in range(len(self.x))]).T\n",
    "    \n",
    "    # 予測分布を出力\n",
    "    def predict_dist(self, x):\n",
    "        K = self.kernel(*np.meshgrid(x, self.x, indexing='ij'))\n",
    "        # 予測分布の平均を計算 PRML式(6.66)\n",
    "        mean = K.dot(self.precision).dot(self.t)\n",
    "        # 予測分布の分散を計算 PRML式(6.67)\n",
    "        var = self.kernel(x, x) + 1 / self.beta - np.sum(K.dot(self.precision) * K, axis=1)\n",
    "        return mean.ravel(), np.sqrt(var.ravel())\n",
    "    \n",
    "    def mean(self,x):\n",
    "        return self.kernels(x).T.dot(np.linalg.inv(self.K + self.beta*np.eye(len(self.x)))).dot(self.y)\n",
    "    \n",
    "    def mean_j(self,x):\n",
    "        return self.kernels_dif(x).T.dot(np.linalg.inv(self.K + self.beta*np.eye(len(self.x)))).dot(self.y)\n",
    "    \n",
    "    def var(self,x):\n",
    "        return self.kernel(x,x)- self.kernels(x).T.dot((np.linalg.inv(self.K)).dot(self.kernels(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import math\n",
    "import random\n",
    "from scipy.optimize.slsqp import approx_jacobian\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GaussianProcessRegression(object):\n",
    "    # カーネル関数とノイズの精度パラメータの初期化\n",
    "    def __init__(self, kernel, beta= 1.0):\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma #論文ではγ\n",
    "        kernelParameter = kernel.get_params()\n",
    "        self.hyperParameter = np.concatenate([kernelParameter,self.beta]).T\n",
    "        \n",
    "        \n",
    "    def setK(self,x,y):\n",
    "        self.K = np.zeros([len(x),len(x)])\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                self.K[i][j] = self.kernel(x[i],x[j])\n",
    "\n",
    "    # カーネル関数のパラメータ推定を行わずに回帰\n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.setK(x,y)\n",
    "\n",
    "    # カーネル関数のパラメータの推定を行う回帰\n",
    "    def fit_kernel(self, x, y, learning_rate=10**(-5), iter_max=5):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.setK(x,y)\n",
    "        self.setC()\n",
    "        for i in range(iter_max):\n",
    "            print(self.hyperParameter)\n",
    "            self.hyperParameter = self.hyperParameter + learning_rate*likelihood()\n",
    "        \n",
    "    def likelihood(self):\n",
    "        CdiffC = np.linalg.inv(self.C).dot(self.calucurateDiffC())\n",
    "        return -0.5*np.trace(CdiffC) + 0.5*self.y.T.dot(CdiffC).dot(np.linalg.inv(self.C)).dot(self.y)\n",
    "    \n",
    "    def calucurateDiffC(self):\n",
    "        diffC = np.zeros((len(self.x),len(self.x),len(self.hyperParameter)))\n",
    "        for i in range(len(self.x)):\n",
    "            for j in range(len(self.x)):\n",
    "                diff1, diff2 =  self.kernel.derivatives(self.x[i],self.x[j])\n",
    "                diffC[i][j][0] = 1 if i==j else 0\n",
    "                diffC[i][j][1] = diff1\n",
    "                diffC[i][j][2] = diff2\n",
    "        return diffC\n",
    "    \n",
    "    def setC(self):\n",
    "        self.C = self.K + self.beta*np.eye(len(self.x))\n",
    "    \n",
    "    def kernels(self,x):\n",
    "        return np.array([self.kernel(x,_x) for _x in Xt]).T\n",
    "    \n",
    "    def kernels_dif(self,x):\n",
    "        params = self.kernel.get_params()\n",
    "        return np.array([- params[1]*(self.x[i]-x)*params[0]**2 *np.exp(-0.5*params[1]*(self.x[i]-x)**2) for i in range(len(self.x))]).T\n",
    "    \n",
    "    # 予測分布を出力\n",
    "    def predict_dist(self, x):\n",
    "        K = self.kernel(*np.meshgrid(x, self.x, indexing='ij'))\n",
    "        # 予測分布の平均を計算 PRML式(6.66)\n",
    "        mean = K.dot(self.precision).dot(self.t)\n",
    "        # 予測分布の分散を計算 PRML式(6.67)\n",
    "        var = self.kernel(x, x) + 1 / self.beta - np.sum(K.dot(self.precision) * K, axis=1)\n",
    "        return mean.ravel(), np.sqrt(var.ravel())\n",
    "    \n",
    "    def mean(self,x):\n",
    "        return self.kernels(x).T.dot(np.linalg.inv(self.K + self.beta*np.eye(len(self.x)))).dot(self.y)\n",
    "    \n",
    "    def mean_j(self,x):\n",
    "        return self.kernels_dif(x).T.dot(np.linalg.inv(self.K + self.beta*np.eye(len(self.x)))).dot(self.y)\n",
    "    \n",
    "    def var(self,x):\n",
    "        return self.kernel(x,x)- self.kernels(x).T.dot((np.linalg.inv(self.K)).dot(self.kernels(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  0.]\n",
      "  [ 0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros((3,3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
